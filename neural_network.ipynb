{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid (logistic) activation function \n",
    "$$G(\\Theta x)=\\frac{1}{1+e^{-\\Theta^Tx}}$$\n",
    "vector of activation nodes j\n",
    "$$a^{(j)}=\n",
    "g(z^j)=\n",
    "g(\\Theta^{(j-1)}a_{j-1})=\n",
    "\\frac{1}{1+e^{-\\Theta^Tx}}$$\n",
    "\n",
    "### cost function $J(\\Theta)$ + regularization\n",
    "$$J(\\theta)= \\frac{1}{m} \\sum\\limits_{i=1}^m \\sum\\limits_{k=1}^K [-y_k^{(i)}\\log{((h_\\theta(x^{(i)}))_k)}-(1-y_k^{(i)})\\log{(1-(h_\\Theta(x^{(i)}))_k)}] \n",
    "+ \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l} \\sum\\limits_{j=1}^{s_l-1} (\\Theta_{j,i}^{(l)})^2$$\n",
    "\n",
    "L= total number of layers in the network\n",
    "\n",
    "sl= number of units (not counting bias unit) in layer l\n",
    "\n",
    "K= number of output units/classes\n",
    "\n",
    "m= number of examples\n",
    "\n",
    "### Regularization\n",
    "- If you have lots of features and little data - overfitting can be a problem, if  the regularization parameter λ is too big, theta is close to zero this results underfitting\n",
    "- Keep all features, but reduce magnitude of parameters θ\n",
    "- Works well when we have a lot of features, each of which contributes a bit to predicting y\n",
    "- Help to avoid overfitting\n",
    "##### suggestions\n",
    "- Do not regularize terms that correspond to the bias\n",
    "\n",
    "### Hipothesis\n",
    "$$h_\\Theta=$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# Machine Learning by Stanford University, using python(numpy,scipy) to implement neural network instead of octave #\n",
    "####################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "from scipy.optimize import fmin_cg #Minimize a function using a nonlinear conjugate gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def sigmoid(x): #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "#    return 1/(1+np.exp(-x))\n",
    "#----------------------- replaced by scipy.special.expit (much faster) -----------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cost function, vectorized regularized logistic regression for one theta\n",
    "\n",
    "def cost_function(theta, X, y, lambda1): #theta, X, y => np.matrix()\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    J,grad=0,np.matrix(np.zeros(n).reshape(n,1))\n",
    "    h_theta=sigmoid(X*theta) #hypothesis\n",
    "    \n",
    "    J=(-( y.T*np.log(h_theta)+(1-y).T*np.log(1-h_theta) ) + lambda1*theta[1:].T *theta[1:]/2)/m #cost function + regularization\n",
    "    \n",
    "    grad= X.T*(h_theta-y)/m #gradient function + regularization for j=0 (feature 0)\n",
    "    grad[1:]+=lambda1*theta[1:]/m #gradient function + regularization for j>0 (feature > 0)\n",
    "    \n",
    "    return [J,grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient checking\n",
    "$$\\frac{\\partial}{\\partial\\Theta} J(\\Theta) \\approx \\frac{J(\\theta+\\epsilon)-J(\\Theta-\\epsilon)}{2\\epsilon}\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial\\Theta_j} J(\\Theta) \\approx \\frac{J(\\Theta_1,\\ldots,\\Theta_j+\\epsilon,\\ldots,\\Theta_n)-J(\\Theta_1,\\ldots,\\Theta_j-\\epsilon,\\ldots,\\Theta_n)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to function call (<ipython-input-4-f35593ff7ac2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f35593ff7ac2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    thetaPlus(i)=theta(i)+epsilon\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
     ]
    }
   ],
   "source": [
    "#gradient checking, return the approximated gradient\n",
    "#theta is a vector np.array.shape=(n,1)\n",
    "def grad_check(theta,epsilon=10**-4):\n",
    "    n=theta.size\n",
    "    for i in range(n):\n",
    "        thetaPlus=theta\n",
    "        thetaPlus(i)=theta(i)+epsilon\n",
    "        thetaMinus=theta\n",
    "        thetaMinus(i)=thetaMinus(i)-epsilon\n",
    "        gradApprox(i)=(cost_function(thetaPlus)[0]-cost_function(thetaMinus)[0])/(2*epsilon)\n",
    "    return gradApprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Minimize a function using a nonlinear conjugate gradient algorithm\n",
    "#------------------------------------------------------------------------\n",
    "#scipy.optimize.fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, \n",
    "#                                                    maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
    "#------------------------------------------------------------------------\n",
    "#f : callable, f(x, *args)\n",
    "# Objective function to be minimized. Here x must be a 1-D array of the variables that are to be changed in \n",
    "# the search for a minimum, and args are the other (fixed) parameters of f.\n",
    "#------------------------------------------------------------------------\n",
    "#x0 : ndarray\n",
    "# A user-supplied initial estimate of xopt, the optimal value of x. It must be a 1-D array of values.\n",
    "#------------------------------------------------------------------------\n",
    "#fprime : callable, fprime(x, *args), optional\n",
    "# A function that returns the gradient of f at x. Here x and args are as described above for f. \n",
    "# The returned value must be a 1-D array. Defaults to None, in which case the gradient is approximated numerically \n",
    "# (see epsilon, below).\n",
    "#------------------------------------------------------------------------\n",
    "#args : tuple, optional\n",
    "# Parameter values passed to f and fprime. Must be supplied whenever additional fixed parameters \n",
    "# are needed to completely specify the functions f and fprime.\n",
    "#------------------------------------------------------------------------\n",
    "#reference => http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html#scipy.optimize.fmin_cg\n",
    "\n",
    "def one_vs_all(X, y, num_labels, lambda1):\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    nk=len(num_labels)#number of classes\n",
    "    all_theta=np.matrix(np.zeros(nk*(n+1))).reshape(nk,n+1) #nk number of classes, n features + X0 (bias)\n",
    "    X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "    for k in num_labels: #for all classes 0-9\n",
    "        args1=(X, y==k, lambda1)\n",
    "        initial_theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "        f1=lambda x,*args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "        fprime1=lambda x,*args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "        theta=fmin_cg( f1,x0=initial_theta,fprime=fprime1,args=args1,maxiter=20 )\n",
    "        #all_theta[k]=theta\n",
    "\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the label for a trained one-vs-all classifier\n",
    "def predict_one_vs_all(all_theta, X):\n",
    "    probability,p=0,0\n",
    "    return [probability,p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK STEPS:\n",
    "\n",
    "1. Pick network architecture:\n",
    "\n",
    "    -number of hidden features should be the same in each layer, the more the better, more than number of features less than 2*features, default use 1 hidden layer\n",
    "\n",
    "2. Training a neural network:\n",
    "\n",
    "    -randomly initialize weigths\n",
    "    \n",
    "    -forward propagation\n",
    "    \n",
    "    -compute cost function\n",
    "    \n",
    "    -backpropagation to compute $$\\frac{\\partial}{\\partial\\Theta^{(l)}_{jk}}J(\\Theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking with rand data\n",
    "lambda1=0.1\n",
    "num_labels=range(10)\n",
    "X=np.matrix(np.loadtxt(\"x\"))\n",
    "y=np.matrix(np.loadtxt(\"y\")).T\n",
    "m=X.shape[0] #number of training values\n",
    "n=X.shape[1] #number of features\n",
    "theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "#X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias) #its made in one-vs-all\n",
    "\n",
    "#one_vs_all(X, y, num_labels, lambda1)\n",
    "\n",
    "\n",
    "\n",
    "nk=len(num_labels)#number of classes\n",
    "all_theta=np.matrix(np.zeros(nk*(n+1))).reshape(nk,n+1) #nk number of classes, n features + X0 (bias)\n",
    "X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "for k in num_labels: #for all classes 0-9\n",
    "    args=(X, y==k, lambda1)\n",
    "    initial_theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "    f=lambda x,args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "    fprime=lambda x,args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "    #theta=fmin_cg( f1,x0=initial_theta,fprime=fprime1,args=args1,maxiter=20 )\n",
    "    #all_theta[k]=theta\n",
    "    if k==2:\n",
    "        print fprime(initial_theta, args)[10]\n",
    "        print cost_function(initial_theta,args[0],args[1],args[2])[1][10]\n",
    "print X.shape,y.shape,theta.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
