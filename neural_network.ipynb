{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# Machine Learning by Stanford University, using python(numpy,scipy) to implement neural network instead of octave #\n",
    "####################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "from scipy.optimize import fmin_cg #Minimize a function using a nonlinear conjugate gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def sigmoid(x): #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "#    return 1/(1+np.exp(-x))\n",
    "#----------------------- replaced by scipy.special.expit (much faster) -----------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cost function, vectorized regularized logistic regression for one theta\n",
    "\n",
    "def cost_function(theta, X, y, lambda1): #theta, X, y => np.matrix()\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    J,grad=0,np.matrix(np.zeros(n).reshape(n,1))\n",
    "    h_theta=sigmoid(X*theta) #hypothesis\n",
    "    \n",
    "    J=(-( y.T*np.log(h_theta)+(1-y).T*np.log(1-h_theta) ) + lambda1*theta[1:].T *theta[1:]/2)/m #cost function + regularization\n",
    "    \n",
    "    grad= X.T*(h_theta-y)/m #gradient function + regularization for j=0 (feature 0)\n",
    "    grad[1:]+=lambda1*theta[1:]/m #gradient function + regularization for j>0 (feature > 0)\n",
    "    \n",
    "    return [J,grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Minimize a function using a nonlinear conjugate gradient algorithm\n",
    "#------------------------------------------------------------------------\n",
    "#scipy.optimize.fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, \n",
    "#                                                    maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
    "#------------------------------------------------------------------------\n",
    "#f : callable, f(x, *args)\n",
    "# Objective function to be minimized. Here x must be a 1-D array of the variables that are to be changed in \n",
    "# the search for a minimum, and args are the other (fixed) parameters of f.\n",
    "#------------------------------------------------------------------------\n",
    "#x0 : ndarray\n",
    "# A user-supplied initial estimate of xopt, the optimal value of x. It must be a 1-D array of values.\n",
    "#------------------------------------------------------------------------\n",
    "#fprime : callable, fprime(x, *args), optional\n",
    "# A function that returns the gradient of f at x. Here x and args are as described above for f. \n",
    "# The returned value must be a 1-D array. Defaults to None, in which case the gradient is approximated numerically \n",
    "# (see epsilon, below).\n",
    "#------------------------------------------------------------------------\n",
    "#args : tuple, optional\n",
    "# Parameter values passed to f and fprime. Must be supplied whenever additional fixed parameters \n",
    "# are needed to completely specify the functions f and fprime.\n",
    "#------------------------------------------------------------------------\n",
    "#reference => http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html#scipy.optimize.fmin_cg\n",
    "\n",
    "def one_vs_all(X, y, num_labels, lambda1):\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    nk=len(num_labels)#number of classes\n",
    "    all_theta=np.matrix(np.zeros(nk*(n+1))).reshape(nk,n+1) #nk number of classes, n features + X0 (bias)\n",
    "    X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "    for k in num_labels: #for all classes 0-9\n",
    "        args1=(X, y==k, lambda1)\n",
    "        initial_theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "        f1=lambda x,*args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "        fprime1=lambda x,*args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "        theta=fmin_cg( f1,x0=initial_theta,fprime=fprime1,args=args1,maxiter=20 )\n",
    "        #all_theta[k]=theta\n",
    "\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the label for a trained one-vs-all classifier\n",
    "def predict_one_vs_all(all_theta, X):\n",
    "    probability,p=0,0\n",
    "    return [probability,p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]]\n",
      "[[ 0.]]\n",
      "(11, 401) (11, 1) (401, 1)\n"
     ]
    }
   ],
   "source": [
    "#checking with rand data\n",
    "lambda1=0.1\n",
    "num_labels=range(10)\n",
    "X=np.matrix(np.loadtxt(\"x\"))\n",
    "y=np.matrix(np.loadtxt(\"y\")).T\n",
    "m=X.shape[0] #number of training values\n",
    "n=X.shape[1] #number of features\n",
    "theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "#X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias) #its made in one-vs-all\n",
    "\n",
    "#one_vs_all(X, y, num_labels, lambda1)\n",
    "\n",
    "\n",
    "\n",
    "nk=len(num_labels)#number of classes\n",
    "all_theta=np.matrix(np.zeros(nk*(n+1))).reshape(nk,n+1) #nk number of classes, n features + X0 (bias)\n",
    "X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "for k in num_labels: #for all classes 0-9\n",
    "    args=(X, y==k, lambda1)\n",
    "    initial_theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "    f=lambda x,args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "    fprime=lambda x,args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "    #theta=fmin_cg( f1,x0=initial_theta,fprime=fprime1,args=args1,maxiter=20 )\n",
    "    #all_theta[k]=theta\n",
    "    if k==2:\n",
    "        print fprime(initial_theta, args)[10]\n",
    "        print cost_function(initial_theta,args[0],args[1],args[2])[1][10]\n",
    "print X.shape,y.shape,theta.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
