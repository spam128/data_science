{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# Machine Learning by Stanford University, using python(numpy,scipy) to implement neural network instead of octave #\n",
    "####################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "from scipy.optimize import fmin_cg #Minimize a function using a nonlinear conjugate gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def sigmoid(x): #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "#    return 1/(1+np.exp(-x))\n",
    "#----------------------- replaced by scipy.special.expit (much faster) -----------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-a7515d85f51d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a7515d85f51d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    theta, X, y, lambda\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#cost function, vectorized regularized logistic regression for one theta\n",
    "\n",
    "def cost_function(theta, X, y, lambda1): #theta, X, y => np.matrix()\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    J,grad=0,np.matrix(np.zeros(n).reshape(n,1))\n",
    "    h_theta=sigmoid(X*theta) #hypothesis\n",
    "    \n",
    "    J=(-( y.T*np.log(h_theta)+(1-y).T*log(1-h_theta) ) + lambda1*theta[1:].T *theta[1:]/2)/m #cost function + regularization\n",
    "    \n",
    "    grad= X.T*(h_theta-y)/m #gradient function + regularization for j=0 (feature 0)\n",
    "    grad[1:]+=lambda1*theta[1:]/m #gradient function + regularization for j>0 (feature > 0)\n",
    "    \n",
    "    return [J,grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minimize a function using a nonlinear conjugate gradient algorithm\n",
    "#------------------------------------------------------------------------\n",
    "#scipy.optimize.fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, \n",
    "#                                                    maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
    "#------------------------------------------------------------------------\n",
    "#f : callable, f(x, *args)\n",
    "# Objective function to be minimized. Here x must be a 1-D array of the variables that are to be changed in \n",
    "# the search for a minimum, and args are the other (fixed) parameters of f.\n",
    "#------------------------------------------------------------------------\n",
    "#x0 : ndarray\n",
    "# A user-supplied initial estimate of xopt, the optimal value of x. It must be a 1-D array of values.\n",
    "#------------------------------------------------------------------------\n",
    "#fprime : callable, fprime(x, *args), optional\n",
    "# A function that returns the gradient of f at x. Here x and args are as described above for f. \n",
    "# The returned value must be a 1-D array. Defaults to None, in which case the gradient is approximated numerically \n",
    "# (see epsilon, below).\n",
    "#------------------------------------------------------------------------\n",
    "#args : tuple, optional\n",
    "# Parameter values passed to f and fprime. Must be supplied whenever additional fixed parameters \n",
    "# are needed to completely specify the functions f and fprime.\n",
    "#------------------------------------------------------------------------\n",
    "#reference => http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html#scipy.optimize.fmin_cg\n",
    "\n",
    "def one_vs_all(X, y, num_labels, lambda1):\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    all_theta=np.zeros(num_labels,n+1) #number of classes, n features + X0 (bias)\n",
    "    X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "    for k in num_labels: #for all classes 0-9\n",
    "        args=(X, y==k, lambda1)\n",
    "        initial_theta=np.zeros(n+1,1)\n",
    "        f=lambda x,*args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "        fprime=lambda x,*args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "        theta=fmin_cg( f, initial_theta,fprime,args,maxiter=1000)\n",
    "        all_theta[k]=theta\n",
    "    return all_theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
