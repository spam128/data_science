{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid (logistic) activation function \n",
    "$$G(\\Theta x)=\\frac{1}{1+e^{-\\Theta^Tx}}$$\n",
    "vector of activation nodes j\n",
    "$$a^{(j)}=\n",
    "g(z^j)=\n",
    "g(\\Theta^{(j-1)}a_{j-1})=\n",
    "\\frac{1}{1+e^{-\\Theta^Tx}}$$\n",
    "\n",
    "### cost function $J(\\Theta)$ + regularization\n",
    "$$J(\\theta)= \\frac{1}{m}\\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^K [-y_k^{(i)}\\log{((h_\\theta(x^{(i)}))_k)}-(1-y_k^{(i)})\\log{(1-(h_\\Theta(x^{(i)}))_k)}] \n",
    "+ \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l} \\sum\\limits_{j=1}^{s_l-1} (\\Theta_{j,i}^{(l)})^2$$\n",
    "\n",
    "L= total number of layers in the network\n",
    "\n",
    "sl= number of units (not counting bias unit) in layer l\n",
    "\n",
    "K= number of output units/classes\n",
    "\n",
    "m= number of examples\n",
    "\n",
    "### Regularization\n",
    "- If you have lots of features and little data - overfitting can be a problem, if  the regularization parameter λ is too big, theta is close to zero this results underfitting\n",
    "- Keep all features, but reduce magnitude of parameters θ\n",
    "- Works well when we have a lot of features, each of which contributes a bit to predicting y\n",
    "- Help to avoid overfitting\n",
    "##### suggestions\n",
    "- Do not regularize terms that correspond to the bias\n",
    "\n",
    "### Hipothesis\n",
    "$$h_\\Theta=a^{(L)}$$\n",
    "### Backpropagation\n",
    "$$D^{(l)}_{i,j}=\\frac{\\partial}{\\partial\\Theta_{i,j}^{(l)}}J(\\Theta)=\\frac{1}{m}\\sum\\limits^m_{t=1}a_j^{(t)(l)}\\partial_i^{(t)(l+1)}=\\partial^{(l+1)(a^{(l)})^T}$$\n",
    "\n",
    "$D^{(l)}_{i,j}=\\frac{1}{m}(\\Delta^{(l)}_{i,j}+\\lambda\\Theta^{(l)}_{i,j})$ if $j\\neq 0$\n",
    "\n",
    "$D^{(l)}_{i,j}=\\frac{1}{m}\\Delta^{(l)}_{i,j}$ if $j=0$\n",
    "\n",
    "L - number of last layer ( how many layers)\n",
    "\n",
    "$\\delta^{(l)}_j$ => error of node j in layer l\n",
    "\n",
    "$\\delta^{(L)}=a^{(L)}-y$ => error of last node\n",
    "\n",
    "$\\delta^{(l)}=((\\Theta^{(l)})^T\\delta^{(l+1)}.*g'(z^{(l)})$ \n",
    "\n",
    "$g'(z^{(l)}=a^{(l)}.*(1-a^{(l)})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# Machine Learning by Stanford University, using python(numpy,scipy) to implement neural network instead of octave #\n",
    "####################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import expit as sigmoid #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "from scipy.optimize import fmin_cg #Minimize a function using a nonlinear conjugate gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def sigmoid(x): #sigmoid function, assigns a number from (0,1) interval, to a real number\n",
    "#    return 1/(1+np.exp(-x))\n",
    "#----------------------- replaced by scipy.special.expit (much faster) -----------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO numpy matrixes shouldnt be stored in python list(a and theta)\n",
    "#cost function, vectorized regularized logistic regression for theta\n",
    "    #nn_params=neural network parameters => matrix with all $/Theta$ needs to be reshaped\n",
    "    #layer=> array with number of elements in every layer without bias\n",
    "def cost_function(nn_params,layer, X, y, lambda1): #theta, X, y => np.matrix()\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    J,grad=0,np.matrix(np.zeros(n).reshape(n,1))\n",
    "    L=len(layer)-1#index of last layer\n",
    "    #checking nn_params\n",
    "    theta_size=[ [layer[i+1],layer[i]+1] for i in xrange(len(layer)-1)]\n",
    "    #reshaping nn_params to thetas\n",
    "    theta=[]#table of thetas\n",
    "    tx=0\n",
    "    for i in xrange(len(theta_size)):\n",
    "        lsize=theta_size[i][0]*theta_size[i][1]\n",
    "        if i==0:\n",
    "            lsize-=1\n",
    "        thetax=np.mat(nn_params[0,tx:tx+lsize+1],dtype=np.float64)\n",
    "        thetax.resize(theta_size[i][0],theta_size[i][1])\n",
    "        tx+=lsize+1\n",
    "        theta.append(thetax)\n",
    "    #feedforward\n",
    "    a=[X]#adding bias column, a1 and a2\n",
    "    for i in xrange(len(theta)):\n",
    "        a[i]=np.hstack((np.ones((a[i].shape[0],1)),a[i]))\n",
    "        a.append(sigmoid(a[i]*theta[i].T))\n",
    "    #calculating cost\n",
    "    J=-np.sum((np.multiply(y,np.log(a[L]))+np.multiply(1-y,np.log(1-a[L]))).flatten())/m\n",
    "    J+=lambda1*(sum([np.sum(np.multiply(th,th).flatten()) for th in theta]))/(2*m)#adding regularization\n",
    "    #backpropagation\n",
    "    \n",
    "    delta=y-a[L]\n",
    "    \n",
    "    \n",
    "    grad= X.T*(h_theta-y)/m #gradient function + regularization for j=0 (feature 0)\n",
    "    grad[1:]+=lambda1*theta[1:]/m #gradient function + regularization for j>0 (feature > 0)\n",
    "    #backpropagation \n",
    "    \n",
    "    return [J,grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient checking\n",
    "$$\\frac{\\partial}{\\partial\\Theta} J(\\Theta) \\approx \\frac{J(\\theta+\\epsilon)-J(\\Theta-\\epsilon)}{2\\epsilon}\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial\\Theta_j} J(\\Theta) \\approx \\frac{J(\\Theta_1,\\ldots,\\Theta_j+\\epsilon,\\ldots,\\Theta_n)-J(\\Theta_1,\\ldots,\\Theta_j-\\epsilon,\\ldots,\\Theta_n)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to function call (<ipython-input-4-f35593ff7ac2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f35593ff7ac2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    thetaPlus(i)=theta(i)+epsilon\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
     ]
    }
   ],
   "source": [
    "#gradient checking, return the approximated gradient\n",
    "#theta is a vector np.array.shape=(n,1)\n",
    "def grad_check(theta,epsilon=10**-4):\n",
    "    n=theta.size\n",
    "    for i in xrange(n):\n",
    "        thetaPlus=theta\n",
    "        thetaPlus(i)=theta(i)+epsilon\n",
    "        thetaMinus=theta\n",
    "        thetaMinus(i)=thetaMinus(i)-epsilon\n",
    "        gradApprox(i)=(cost_function(thetaPlus)[0]-cost_function(thetaMinus)[0])/(2*epsilon)\n",
    "    return gradApprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Minimize a function using a nonlinear conjugate gradient algorithm\n",
    "#------------------------------------------------------------------------\n",
    "#scipy.optimize.fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, \n",
    "#                                                    maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
    "#------------------------------------------------------------------------\n",
    "#f : callable, f(x, *args)\n",
    "# Objective function to be minimized. Here x must be a 1-D array of the variables that are to be changed in \n",
    "# the search for a minimum, and args are the other (fixed) parameters of f.\n",
    "#------------------------------------------------------------------------\n",
    "#x0 : ndarray\n",
    "# A user-supplied initial estimate of xopt, the optimal value of x. It must be a 1-D array of values.\n",
    "#------------------------------------------------------------------------\n",
    "#fprime : callable, fprime(x, *args), optional\n",
    "# A function that returns the gradient of f at x. Here x and args are as described above for f. \n",
    "# The returned value must be a 1-D array. Defaults to None, in which case the gradient is approximated numerically \n",
    "# (see epsilon, below).\n",
    "#------------------------------------------------------------------------\n",
    "#args : tuple, optional\n",
    "# Parameter values passed to f and fprime. Must be supplied whenever additional fixed parameters \n",
    "# are needed to completely specify the functions f and fprime.\n",
    "#------------------------------------------------------------------------\n",
    "#reference => http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html#scipy.optimize.fmin_cg\n",
    "\n",
    "def one_vs_all(X, y, num_labels, lambda1):\n",
    "    m=X.shape[0] #number of training values\n",
    "    n=X.shape[1] #number of features\n",
    "    nk=len(num_labels)#number of classes\n",
    "    all_theta=np.matrix(np.zeros(nk*(n+1))).reshape(nk,n+1) #nk number of classes, n features + X0 (bias)\n",
    "    X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "    for k in num_labels: #for all classes 0-9\n",
    "        args1=(X, y==k, lambda1)\n",
    "        initial_theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "        f1=lambda x,*args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "        fprime1=lambda x,*args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "        theta=fmin_cg( f1,x0=initial_theta,fprime=fprime1,args=args1,maxiter=20 )\n",
    "        #all_theta[k]=theta\n",
    "\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the label for a trained one-vs-all classifier\n",
    "def predict_one_vs_all(all_theta, X):\n",
    "    probability,p=0,0\n",
    "    return [probability,p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK STEPS:\n",
    "\n",
    "1. Pick network architecture:\n",
    "\n",
    "    -number of hidden features should be the same in each layer, the more the better, more than number of features less than 2*features, default use 1 hidden layer\n",
    "\n",
    "2. Training a neural network:\n",
    "\n",
    "    -randomly initialize weigths\n",
    "    \n",
    "    -forward propagation\n",
    "    \n",
    "    -compute cost function\n",
    "    \n",
    "    -backpropagation to compute $$\\frac{\\partial}{\\partial\\Theta^{(l)}_{jk}}J(\\Theta)$$\n",
    "    -compute $min_{\\Theta} J(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa3aad43dcf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlambda1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#number of training values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#checking with rand data\n",
    "lambda1=0.1\n",
    "num_labels=range(10)\n",
    "X=np.matrix(np.loadtxt(\"x\"))\n",
    "y=np.matrix(np.loadtxt(\"y\")).T\n",
    "m=X.shape[0] #number of training values\n",
    "n=X.shape[1] #number of features\n",
    "theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "#X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias) #its made in one-vs-all\n",
    "\n",
    "#one_vs_all(X, y, num_labels, lambda1)\n",
    "\n",
    "\n",
    "\n",
    "nk=len(num_labels)#number of classes\n",
    "all_theta=np.matrix(np.zeros(nk*(n+1))).reshape(nk,n+1) #nk number of classes, n features + X0 (bias)\n",
    "X=np.insert(X,0,1,axis=1)#add column 0 with values 1 (bias)\n",
    "for k in num_labels: #for all classes 0-9\n",
    "    args=(X, y==k, lambda1)\n",
    "    initial_theta=np.matrix(np.zeros(n+1)).reshape(n+1,1)\n",
    "    f=lambda x,args: cost_function(x,args[0],args[1],args[2])[0] #minimalize cost function, search theta (x=theta)\n",
    "    fprime=lambda x,args:cost_function(x,args[0],args[1],args[2])[1] #return gradient for given theta (x=theta)\n",
    "    #theta=fmin_cg( f1,x0=initial_theta,fprime=fprime1,args=args1,maxiter=20 )\n",
    "    #all_theta[k]=theta\n",
    "    if k==2:\n",
    "        print fprime(initial_theta, args)[10]\n",
    "        print cost_function(initial_theta,args[0],args[1],args[2])[1][10]\n",
    "print X.shape,y.shape,theta.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test with data from coursera, recognize digits using neural network\n",
    "\n",
    "#load x,y\n",
    "a=scipy.io.loadmat(\"ex4data1.mat\")\n",
    "x=np.asmatrix(a[\"X\"],dtype=np.float64)\n",
    "y=np.asmatrix(a[\"y\"],dtype=int)\n",
    "#load theta for 3 layer neural network\n",
    "a=scipy.io.loadmat(\"ex4weights.mat\")\n",
    "Theta1=np.asmatrix(a[\"Theta1\"],dtype=np.float64)\n",
    "Theta2=np.asmatrix(a[\"Theta2\"],dtype=np.float64)\n",
    "del a\n",
    "lambda1=0.1\n",
    "\n",
    "nn_params=np.concatenate((Theta1.flatten(),Theta2.flatten()),axis=1) #convert params to vector 1x [Theta1.size+Theta2.size]\n",
    "layer=[x.shape[1], 25,  10]\n",
    "#change y to vector for example for y=1 [0 1 0 0 0 0 0 0 0 0]\n",
    "y2=np.zeros((5000,10),dtype=np.float64)#5000 examples 10 numbers\n",
    "for i in xrange(5000):# 0 as 10, indices -1 (data prepared for octave)\n",
    "    numb=y[i]-1\n",
    "    y2[i,numb]=1\n",
    "        \n",
    "cost_function(nn_params,layer, X, y, lambda1): #theta, X, y => np.matrix()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
